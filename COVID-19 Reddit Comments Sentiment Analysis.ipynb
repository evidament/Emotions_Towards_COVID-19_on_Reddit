{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis of Reddit's COVID-19 Comments\n***This is an ongoing project*** -- \n*Last update: 15th of April 2020* \n\n### Table of contents\n\n1. [Aknowledgements](#1)\n2. [Introduction](#2)\n3. [Data](#3)\n4. [Analysis](#4)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n## Aknowledgements\n\nAs a new-comer in the field of data science I humbly observe all the work that has been done before and thank those who created it as it has helped me to learn so much and start to work on my own projects.\n\nSpecial thanks to [Duarte O.Carmo](https://pbpython.com/interactive-dashboards.html),  whose analysis using Pushshift API has inspired this analysis, [Lorraine Li](https://towardsdatascience.com/k-means-clustering-with-scikit-learn-6b47a369a83c) for her excellent article on K-means clustering,  [Jason Wei](https://github.com/jasonwei20/eda_nlp) for the implementation of data augmenting techniques, which I used to enrich the dataset used for training the classification model and [Aditya Vivek Thota](https://medium.com/the-research-nest/applied-machine-learning-part-3-3fd405842a18) for his great article on machine learning classification for NLP.\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n## Introduction\n\nThis is a simple analysis of the sentiment of Reddit user's comments related to the disease starting from the 31st of December 2019, when the first cases were declared in China.\n\nAs the coronavirus has been expanding all over the world, citizens of every country have given their opinion of the current situation and events. This notebook is a first approach to the question of how the people's opinions have been changing over time.\n\nReddit platform of social sharing, which contents and comments are voted by its community, might show some light to answer this question."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n## Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's import all the necessary libraries for this project\n\nimport requests\nimport numpy as np\nimport pandas as pd\nimport json\nfrom datetime import datetime\nimport plotly.express as px\nimport textblob\nfrom textblob.classifiers import NaiveBayesClassifier\nimport nltk\nimport sklearn\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to retrieve the comments made on Reddit containing the word 'coronavirus' for each day since the start of the disease until the date of today."},{"metadata":{"trusted":true},"cell_type":"code","source":"# epoch time corresponding to Tuesday, 31 December 2019 0:00:00 GMT\nstart_date = 1577750400\nnow = datetime.now()\nnow_epoch = now.timestamp()\nend_date = int(now_epoch)\ndf_final = pd.DataFrame()\n\ndef get_pushshift_data(base_url, **kwargs):\n    base_url = 'https://api.pushshift.io/reddit/search/comment/?'\n    params = kwargs\n    request = requests.get(base_url, params=kwargs)\n    return request.json()\n\nwhile start_date <= end_date:\n    data = get_pushshift_data(base_url = 'https://api.pushshift.io/reddit/search/comment/?',\n                              q='coronavirus',\n                              after=str(start_date),\n                              before=str(start_date+86400), #86400 is the equivalent of 24 hours\n                              size=500,\n                              sort_type='score',\n                              sort='desc').get('data')\n    if data == None:\n        start_date = start_date + 86400\n    else:\n        df = pd.DataFrame.from_records(data)\n        df_final= df_final.append(df)\n        start_date = start_date + 86400\n\n# Let's keep only some columns for our final database\ndf_final = df_final[['created_utc', 'author', 'subreddit', 'body', 'score', 'permalink']]\n\n# Let's create a new column with all the epoch dates converted\ndf_final['date'] = pd.to_datetime(df_final['created_utc'], unit='s')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to use the NLP library Textblob for our first approach into a Sentiment Analysis of our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import textblob\n\n# create a column with sentiment polarity\ndf_final[\"sentiment_polarity\"] = df_final.apply(lambda row: textblob.TextBlob(row[\"body\"]).sentiment.polarity, axis=1)\n\n# create a column with sentiment subjectivity\ndf_final[\"sentiment_subjectivity\"] = df_final.apply(lambda row: textblob.TextBlob(row[\"body\"]).sentiment.subjectivity, axis=1)\n\n# create a column with 'positive' or 'negative' depending on sentiment_polarity\ndf_final[\"sentiment\"] = df_final.apply(lambda row: \"positive\" if row[\"sentiment_polarity\"] >= 0 else \"negative\", axis=1)\n\n# create a column with a text preview that shows the first 50 characters\ndf_final[\"preview\"] = df_final[\"body\"].str[0:50]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a preview of the final dataset, containing new columns created after the sentiment analysis of Reddit's comments has been done."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.to_csv(path_or_buf='covid19_comments.csv')\ndf_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data cleaning\n\nLet's check if there are any null values in our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('covid19_comments.csv').isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it might happen that some comments may have a negative score and this would have bad consequences in the visualization of our data, we are going to check if some values are negative within our dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.groupby('score')['score'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's replace negative values for a positive value of 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final['score'].replace(to_replace=-1, value=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n## Analysis\n\n### Exploratory Data Analysis\n\nThis is a first visualization of all the data in our dataframe using Plotly Express."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(df_final, x=\"date\", \n           y=\"sentiment_polarity\", \n           hover_data=[\"author\", \"permalink\", \"preview\"], \n           color_discrete_sequence=[\"green\", \"red\"], \n           color=\"sentiment\", \n           size=\"score\", \n           size_max=150,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Reddit's COVID-19 related comments' sentiment over time\", \n          )\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white'\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As this visualization may seem to complex to understand, we are going to split the comments we retrieved from Reddit for each month they were created.\n\nFirst, we can see below these lines the distribution of comments from the first month of 2020 until now."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final['month'] = pd.DatetimeIndex(df_final['date']).month\ndf_final.groupby('month')['month'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the distribution of positive and negative comments in each month of 2020."},{"metadata":{"trusted":true},"cell_type":"code","source":"january_comments = df_final.loc[df_final['month'] == 1]\njanuary_comments.to_csv(path_or_buf='january_comments.csv')\n\nfebruary_comments = df_final.loc[df_final['month'] == 2]\nfebruary_comments.to_csv(path_or_buf='february_comments.csv')\n\nmarch_comments = df_final.loc[df_final['month'] == 3]\nmarch_comments.to_csv(path_or_buf='march_comments.csv')\n\napril_comments = df_final.loc[df_final['month'] == 4]\napril_comments.to_csv(path_or_buf='april_comments.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(january_comments, x=\"date\", \n           y=\"sentiment_polarity\",\n           hover_data=[\"author\", \"permalink\", \"preview\"], \n           color_discrete_sequence=[\"green\", \"red\"],\n           color=\"sentiment\", \n           size=\"score\", \n           size_max=50,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Reddit's COVID-19 related comments' sentiment during January 2020\")\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white'\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(february_comments, x=\"date\", \n           y=\"sentiment_polarity\",\n           hover_data=[\"author\", \"permalink\", \"preview\"],\n           color_discrete_sequence=[\"green\", \"red\"], \n           color=\"sentiment\", \n           size=\"score\", \n           size_max=50,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Reddit's COVID-19 related comments' sentiment on February 2020\",\n          )\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white'\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(march_comments, x=\"date\",\n           y=\"sentiment_polarity\",\n           hover_data=[\"author\", \"permalink\", \"preview\"], \n           color_discrete_sequence=[\"green\", \"red\"], \n           color=\"sentiment\", \n           size=\"score\", \n           size_max=50,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Reddit's COVID-19 related comments' sentiment on March 2020\",\n          )\nfig.update_layout(\nautosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(april_comments, x=\"date\",\n           y=\"sentiment_polarity\",\n           hover_data=[\"author\", \"permalink\", \"preview\"], \n           color_discrete_sequence=[\"green\", \"red\"], \n           color=\"sentiment\", \n           size=\"score\", \n           size_max=50,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Reddit's COVID-19 related comments' sentiment on April 2020\",\n          )\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white'\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clustering\n\n\nLet's use K-means clustering to clasify the comments in different clusters. First we are going to use the Elbow Method to determine the best value for *k*."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_km = pd.DataFrame(df_final['score'])\ndf_km['sentiment_polarity'] = df_final['sentiment_polarity']\n\n# ELBOW METHOD\n\n# calculate distortion for a range of number of cluster\ndistortions = []\nfor i in range(1, 11):\n    km = KMeans(\n        n_clusters=i, init='random',\n        n_init=10, max_iter=300,\n        tol=1e-04, random_state=0\n    )\n    km.fit(df_km)\n    distortions.append(km.inertia_)\n\n# plot\nplt.plot(range(1, 11), distortions, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Distortion')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kclusters = 4\n\nkm = KMeans(n_clusters=kclusters, init='random',n_init=10, max_iter=300, \n    tol=1e-04, random_state=0).fit(df_km)\n\nkm_labels = km.labels_\n\n# Let's change label numbers so they go from highest scores to lowest\n\nreplace_labels = {0:2, 1:0, 2:3, 3:1}\n\nfor i in range(len(km_labels)):\n    km_labels[i] = replace_labels[km_labels[i]]\n    \ndf_km['Cluster'] = km_labels\ndf_km.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see which cluster has the comments with highest scores, regardless of their sentiment polarity."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.ticker as ticker\n\nfig, axes = plt.subplots(1, kclusters, figsize=(20, 10), sharey=True)\n\naxes[0].set_ylabel('Score & Sentiment Polarity', fontsize=25)\n\nfor k in range(kclusters):\n    # We are going to set same y axis limits\n    axes[k].set_ylim(-1,500)\n    axes[k].xaxis.set_label_position('top')\n    axes[k].set_xlabel('Cluster ' + str(k), fontsize=25)\n    axes[k].tick_params(labelsize=20)\n    plt.sca(axes[k])\n    plt.xticks(rotation='vertical')\n    sns.boxplot(data = df_km[df_km['Cluster'] == k].drop('Cluster',1), ax=axes[k])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have now the most rated comments, we can plot a graphic to see if there have been more positive or negative comments regarding the coronavirus."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final['Cluster'] = df_km['Cluster']\n\ndf_cluster0 = df_final.loc[df_final['Cluster'] == 0]\n\nfig = px.scatter(df_cluster0, x=\"date\",\n           y=\"sentiment_polarity\", \n           trendline = 'lowess',\n           hover_data=[\"author\", \"permalink\", \"preview\"], \n           color_discrete_sequence=[\"green\", \"red\"], \n           color=\"sentiment\", \n           size=\"score\", \n           size_max=50,\n           labels={\"sentiment_polarity\": \"Comment positivity\", \"date\": \"Date comment was posted\"}, \n           title=f\"Sentiment of Reddit's most-voted comments on COVID-19\",\n          )\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height = 800,\n    plot_bgcolor = 'white'\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Emotion detection\nWe are going to analyse in more precision the emotions we can detect from the comments made regarding the coronavirus training a machine learning model.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from distutils.dir_util import copy_tree\nfromDirectory = '../input'\ntoDirectory = '../output/working/'\ncopy_tree(fromDirectory, toDirectory)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../output/working/text_emotion.csv')\n\n# Let's drop unnecessary columns from our dataset\ndata = data.drop('tweet_id', axis=1)\ndata = data.drop('author', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find out the unique sentiment values in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['sentiment'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the sake of simplicity, we will only keep 4 of the emotions that align with psychologist Paul Ekman's classification of basic emotions: 'happiness', 'sadness', 'anger' and 'fear' (fear equates to 'worry' in our dataset)."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(data[data.sentiment == 'boredom'].index)\ndata = data.drop(data[data.sentiment == 'surprise'].index)\ndata = data.drop(data[data.sentiment == 'enthusiasm'].index)\ndata = data.drop(data[data.sentiment == 'empty'].index)\ndata = data.drop(data[data.sentiment == 'fun'].index)\ndata = data.drop(data[data.sentiment == 'relief'].index)\ndata = data.drop(data[data.sentiment == 'love'].index)\ndata = data.drop(data[data.sentiment == 'neutral'].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to merge the category 'hate' to the 'anger' category to make a data augmentation\ndata['sentiment'].replace(to_replace='hate', value='anger', inplace=True)\n# Let's replace 'worry' for 'fear'\ndata['sentiment'].replace(to_replace='worry', value='fear', inplace=True)\n# Let's see how many tweets we have for each sentiment\ndata.groupby('sentiment')['sentiment'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elements in 'anger' category are few in comparison to other categories. Let's do a data augmentation in that category in order to balance all the categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"anger = data.loc[data['sentiment'] == 'anger']\n#happiness = data.loc[data['sentiment'] == 'happiness']\n#sadness = data.loc[data['sentiment'] == 'sadness']\n#fear = data.loc[data['sentiment'] == 'fear']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Synonym replacement\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nimport random\n\ndef get_synonyms(word):\n    \"\"\"\n    Get synonyms of a word\n    \"\"\"\n    synonyms = set()\n    \n    for syn in wordnet.synsets(word): \n        for l in syn.lemmas(): \n            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n            synonyms.add(synonym) \n    \n    if word in synonyms:\n        synonyms.remove(word)\n    \n    return list(synonyms)\n\ndef synonym_replacement(words, n):\n    \n    words = words.split()\n    \n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if word not in stop]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    \n    for random_word in random_word_list:\n        synonyms = get_synonyms(random_word)\n        \n        if len(synonyms) >= 1:\n            synonym = random.choice(list(synonyms))\n            new_words = [synonym if word == random_word else word for word in new_words]\n            num_replaced += 1\n        \n        if num_replaced >= n: #only replace up to n words\n            break\n\n    sentence = ' '.join(new_words)\n\n    return sentence\n\n# Random Insertion\n\ndef random_insertion(words, n):\n    \n    words = words.split()\n    new_words = words.copy()\n    \n    for _ in range(n):\n        add_word(new_words)\n        \n    sentence = ' '.join(new_words)\n    return sentence\n\ndef add_word(new_words):\n    \n    synonyms = []\n    counter = 0\n    \n    while len(synonyms) < 1:\n        random_word = new_words[random.randint(0, len(new_words)-1)]\n        synonyms = get_synonyms(random_word)\n        counter += 1\n        if counter >= 10:\n            return\n        \n    random_synonym = synonyms[0]\n    random_idx = random.randint(0, len(new_words)-1)\n    new_words.insert(random_idx, random_synonym)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_anger_comments = []\nfor content in anger['content']:\n    new_anger_comments.append(synonym_replacement(content, 4))\n    new_anger_comments.append(random_insertion(content, 4))\nnew_anger = pd.DataFrame()\nnew_anger['content'] = new_anger_comments\nnew_anger['sentiment'] = 'anger'\nanger = anger.append(new_anger)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'Anger' category has now tripled in size which will help to have more accuracy for the machine learning classification algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"anger.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.append(new_anger)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making all letters lowercase\ndata['content'] = data['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n\n#Removing Punctuation, Symbols\ndata['content'] = data['content'].str.replace('[^\\w\\s]',' ')\n\n#Removing urls, links\ndata['content'] = data['content'].str.replace('(www|http)\\S+', ' ')\n\n#Removing Stop Words using NLTK\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndata['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lemmatisation\nfrom textblob import Word\ndata['content'] = data['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\n#Correcting Letter Repetitions\nimport re\n\ndef de_repeat(text):\n    pattern = re.compile(r\"(.)\\1{2,}\")\n    return pattern.sub(r\"\\1\\1\", text)\n\ndata['content'] = data['content'].apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code to find the top 1000 rarest words appearing in the data\nfreq = pd.Series(' '.join(data['content']).split()).value_counts()[-1000:]\n\n# Removing all those rarely appearing words from the data\nfreq = list(freq.index)\ndata['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding output labels\nfrom sklearn import preprocessing\nlbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(data.sentiment.values)\n\n# Splitting into training and testing data in 90:10 ratio\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lbl_enc.classes_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The index of each label is the value given by the encoder: \n\n**anger = 0, fear = 1, happiness = 2, sadness = 3**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting TF-IDF parameters\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_val_tfidf = tfidf.fit_transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting Count Vectors Parameters\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(analyzer='word')\ncount_vect.fit(data['content'])\nX_train_count =  count_vect.transform(X_train)\nX_val_count =  count_vect.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n# Model 1: Multinomial Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train_tfidf, y_train)\ny_pred = nb.predict(X_val_tfidf)\nprint('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 2: Linear SVM\nfrom sklearn.linear_model import SGDClassifier\nlsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\nlsvm.fit(X_train_tfidf, y_train)\ny_pred = lsvm.predict(X_val_tfidf)\nprint('svm using tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 3: logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1, max_iter=21000)\nlogreg.fit(X_train_tfidf, y_train)\ny_pred = logreg.predict(X_val_tfidf)\nprint('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 4: Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(X_train_tfidf, y_train)\ny_pred = rf.predict(X_val_tfidf)\nprint('random forest tfidf accuracy %s' % accuracy_score(y_pred, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Model 1: Multinomial Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train_count, y_train)\ny_pred = nb.predict(X_val_count)\nprint('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 2: Linear SVM\nfrom sklearn.linear_model import SGDClassifier\nlsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\nlsvm.fit(X_train_count, y_train)\ny_pred = lsvm.predict(X_val_count)\nprint('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 3: Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1, max_iter=21000)\nlogreg.fit(X_train_count, y_train)\ny_pred = logreg.predict(X_val_count)\nprint('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n\n# Model 4: Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(X_train_count, y_train)\ny_pred = rf.predict(X_val_count)\nprint('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear SMV seems to be the algorithm with highest accuracy with a 59% using count vectors. Let's use it for our Reddit data and see how many comments fit to every emotion."},{"metadata":{"trusted":true},"cell_type":"code","source":"comments = df_final['body']\n\n# Doing some preprocessing on these comments as done before\ncomments = comments.apply(lambda x: \" \".join(x.lower() for x in x.split()))\ncomments = comments.str.replace('[^\\w\\s]',' ')\ncomments = comments.str.replace('(www|http)\\S+', ' ')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ncomments = comments.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\nfrom textblob import Word\ncomments = comments.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ncomments = comments.apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))\n# Extracting Count Vectors feature from our Reddit comments\ncomment_count = count_vect.transform(comments)\n\n#Predicting the emotion of the comment using our already trained logistic regression\ncomment_pred = lsvm.predict(comment_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final['emotion'] = comment_pred.tolist()\ndf_final['emotion'].replace(to_replace=0, value='anger', inplace=True)\ndf_final['emotion'].replace(to_replace=1, value='fear', inplace=True)\ndf_final['emotion'].replace(to_replace=2, value='happiness', inplace=True)\ndf_final['emotion'].replace(to_replace=3, value='sadness', inplace=True)\ndf_final.groupby('emotion')['emotion'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's group comments by emotion and figure out how many have been made each day."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's group comments by emotion\nanger_comments = df_final.loc[df_final['emotion'] == 'anger']\nfear_comments = df_final.loc[df_final['emotion'] == 'fear']\nhappiness_comments = df_final.loc[df_final['emotion'] == 'happiness']\nsadness_comments = df_final.loc[df_final['emotion'] == 'sadness']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's now figure out the number of comments made per day\ndef groupbydate(dataset):\n    newdataset = pd.DataFrame()\n    dates = []\n    sums = []\n    dataset['date'] = pd.to_datetime(dataset['date']).dt.date\n    for date in dataset['date']:\n        if date not in dates:\n            dates.append(date)\n            sums.append((dataset['date'] == date).sum())\n    newdataset['date'] = dates\n    newdataset['comments'] = sums\n    return newdataset\n\nanger_progression = groupbydate(anger_comments)\nfear_progression = groupbydate(fear_comments)\nhappiness_progression = groupbydate(happiness_comments)\nsadness_progression = groupbydate(sadness_comments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure() \n\nfig.add_trace(go.Scatter(x=anger_progression[\"date\"], y=anger_progression['comments'], \n                        name= 'Anger'))\nfig.add_trace(go.Scatter(x=fear_progression[\"date\"], y=fear_progression['comments'],\n                        name = 'Fear'))\nfig.add_trace(go.Scatter(x=happiness_progression[\"date\"], y=happiness_progression['comments'],\n                        name = 'Happiness'))\nfig.add_trace(go.Scatter(x=sadness_progression[\"date\"], y=sadness_progression['comments'],\n                        name = 'Sadness'))\n\nfig.update_layout(title='Emotions towards COVID-19 on Reddit',\n                   xaxis_title='Date',\n                   yaxis_title='Number of Comments')\n\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}